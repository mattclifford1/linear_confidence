{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "import deltas\n",
    "from deltas.pipeline import data, classifier, evaluation\n",
    "from deltas.model import downsample\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# np.random.seed(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/100 [37:29<61:51:14, 2249.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 2/100 [1:14:45<61:01:10, 2241.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 3/100 [1:52:13<60:28:44, 2244.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "\n",
    "costs = (1, 1)  # change for (1, 10) to increase results\n",
    "dataset = 'MIMIC-III-mortality'\n",
    "# model = 'SVM-rbf'\n",
    "# model = 'MLP-deep'\n",
    "model = 'MIMIC'\n",
    "\n",
    "dfs = []\n",
    "len_required = 10\n",
    "for i in tqdm(range(len_required*10)):\n",
    "    # np.random.seed(random.randint)\n",
    "    # np.random.seed(i)\n",
    "    data_clf = data.get_real_dataset(dataset, _print=False, seed=i, scale=True)\n",
    "    # data_clf = data.get_real_dataset(dataset, _print=False, scale=True)\n",
    "    classifiers_dict = classifier.get_classifier(\n",
    "        data_clf=data_clf,\n",
    "        model=model,\n",
    "        _plot=False,\n",
    "        _print=False)\n",
    "    data_clf['clf'] = classifiers_dict['Baseline']\n",
    "    X = data_clf['data']['X']\n",
    "    y = data_clf['data']['y']\n",
    "    clf = data_clf['clf']\n",
    "    # deltas_model = downsample.downsample_deltas(\n",
    "    #     clf).fit(X, y, _print=True, _plot=True, max_trials=10000)\n",
    "    # deltas_model = base.base_deltas(\n",
    "    #     clf).fit(X, y, grid_search=True, _print=True, _plot=True)\n",
    "    if False:\n",
    "        param_grid = {\n",
    "                    #   'alpha': [0, 0.1, 1, 10],\n",
    "                    #   'grid_search': [True, False],\n",
    "                    'method': ['supports-prop-update_mean', 'supports-prop-update_mean-margin_only']}\n",
    "        grid_original = GridSearchCV(\n",
    "            downsample.downsample_deltas(), param_grid, refit=True)\n",
    "        grid_original.fit(X, y,\n",
    "                        clf=clf,\n",
    "                        _print=False,\n",
    "                        _plot=False,\n",
    "                        max_trials=10000,\n",
    "                        parallel=True)\n",
    "        deltas_model = grid_original.best_estimator_\n",
    "        print(f'Best params: {grid_original.best_params_}')\n",
    "    else:\n",
    "        deltas_model = downsample.downsample_deltas(clf).fit(X, y,\n",
    "                                                             alpha=1,\n",
    "                                                             _print=False,\n",
    "                                                             _plot=False,\n",
    "                                                             max_trials=10000,\n",
    "                                                             parallel=True)\n",
    "\n",
    "    if deltas_model.is_fit == True:\n",
    "        classifiers_dict['Our Method'] = deltas_model\n",
    "        scores_df = evaluation.eval_test(classifiers_dict,\n",
    "                                         data_clf['data_test'], _print=False, _plot=False)\n",
    "        dfs.append(scores_df)\n",
    "    print(len(dfs))\n",
    "    # else:\n",
    "    #     print('not fit deltas')\n",
    "    if len(dfs) == len_required:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(dfs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat(dfs, axis=0)\n",
    "mean = {}\n",
    "std = {}\n",
    "index = df.index.unique().to_list()\n",
    "cols = df.columns.to_list()\n",
    "for method in index:\n",
    "    mean[method] = df.loc[method].mean().to_list()\n",
    "    std[method] = df.loc[method].std().to_list()\n",
    "\n",
    "mean_df = pd.DataFrame.from_dict(mean, orient='index', columns=cols)\n",
    "std_df = pd.DataFrame.from_dict(std, orient='index', columns=cols)\n",
    "print(f'% {dataset} - {len_required}')\n",
    "\n",
    "m = mean_df.to_dict('list')\n",
    "s = std_df.to_dict('list')\n",
    "metrics = mean_df.columns.to_list()\n",
    "methods = mean_df.index.to_list()\n",
    "sf = 5\n",
    "for metric in metrics:\n",
    "    means = m[metric]\n",
    "    sts = s[metric]\n",
    "    mx = np.argmax(means)\n",
    "    for i in range(len(means)):\n",
    "        m_str = str(means[i])[1:sf]\n",
    "        if i == mx:\n",
    "            m_str = f\"\\\\textbf{{{m_str}}}\"\n",
    "        s_str = str(sts[i])[1:sf-1]\n",
    "        m[metric][i] = f'${m_str} \\\\pm {s_str}$'\n",
    "\n",
    "method_map = {\n",
    "    'Baseline': 'Baseline',\n",
    "    'SMOTE': \"SMOTE \\cite{Chawla_2002_JAIR}\",\n",
    "    'Balanced Weights': 'BW',\n",
    "    'BMR': 'BMR \\cite{Bahnsen_2014_SIAM}',\n",
    "    'Threshold': 'Thresh \\cite{Sheng_2006_AAAI}',\n",
    "    'Our Method': 'Our Method',\n",
    "}\n",
    "meths_new = []\n",
    "for me in methods:\n",
    "    meths_new.append(method_map[me])\n",
    "m['Methods'] = meths_new\n",
    "df = pd.DataFrame(m)  # .set_index('Methods')\n",
    "meths = df.pop('Methods')\n",
    "df.insert(0, 'Methods', meths)\n",
    "latex_str = df.to_latex(index=False)\n",
    "\n",
    "# print('\\\\begin{tabular}{@{}lrrrrr@{}}'+latex_str[23:])\n",
    "print(mean_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.now()\n",
    "dt_string = now.strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "save_file = os.path.join(os.path.dirname(\n",
    "    os.path.abspath('')), 'notebooks-ECAI', 'results', f'Results-5-{dataset}.txt')\n",
    "with open(save_file, \"w\") as text_file:\n",
    "    text_file.write(f'% {dt_string}\\n')\n",
    "    text_file.write(f'% {dataset} - {len_required} runs {model} model\\n')\n",
    "    train = np.unique(data_clf['data']['y'], return_counts=True)[1]\n",
    "    test = np.unique(data_clf['data_test']['y'], return_counts=True)[1]\n",
    "    text_file.write(f'% train:{train}, test:{test}\\n')\n",
    "    format = '\\\\begin{tabular}{@{}l' + 'c'*len(mean_df.columns) + '@{}}'\n",
    "    text_file.write(format+latex_str[18+len(mean_df.columns):])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deltas",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
